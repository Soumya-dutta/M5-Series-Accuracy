{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec3jvRxxTvr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "785f09c4-5eff-4010-eb1c-706a02f6fb96"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "dataset_path = '/content/gdrive/My Drive/M5_forecasting/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxo08oEhYs7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  datetime import datetime, timedelta\n",
        "import gc\n",
        "import numpy as np, pandas as pd\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n4SlNCQihmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "908bcfa2-22f4-481c-c178-69a45995de2a"
      },
      "source": [
        "lgb.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYfpV7hKBOlN",
        "colab_type": "text"
      },
      "source": [
        "The different categories of the columns are mentioned \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mwt9tiAY_7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
        "         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
        "        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"int16\", 'snap_TX': 'int16', 'snap_WI': 'int16' }\n",
        "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\"}\n",
        "#PRICE_DTYPES = {\"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OycnEqMYZBlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_columns = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHygRwgTBV1B",
        "colab_type": "text"
      },
      "source": [
        "The starting date of the prediction task is specified along with the last day of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAtfMYDAdTXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16ef27bd-b1ec-4460-8281-db087db788c9"
      },
      "source": [
        "h = 28 \n",
        "max_lags = 57\n",
        "tr_last = 1941\n",
        "fday = datetime(2016,5, 23) \n",
        "#tr_last = 1913\n",
        "#fday = datetime(2016,4, 25) \n",
        "fday"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2016, 5, 23, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LL2RxBFBhxX",
        "colab_type": "text"
      },
      "source": [
        "The three csv's are combined into a single dataframe. Since in the sales_train_evaluation.csv, we have the dates as columns and in the other two we have dates as rows, we melt the dataframe to get dates as rows for each of the three csv files. After this we can merge the 3 csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9drYmieu0Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dt(is_train = True, nrows = None, first_day = 1200):\n",
        "    prices = pd.read_csv(dataset_path+\"sell_prices.csv\", dtype = PRICE_DTYPES)\n",
        "    for col, col_dtype in PRICE_DTYPES.items():\n",
        "        if col_dtype == \"category\":\n",
        "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
        "            prices[col] -= prices[col].min()\n",
        "            \n",
        "    cal = pd.read_csv(dataset_path+\"calendar.csv\", dtype = CAL_DTYPES)\n",
        "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
        "    for col, col_dtype in CAL_DTYPES.items():\n",
        "        if col_dtype == \"category\":\n",
        "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
        "            cal[col] -= cal[col].min()\n",
        "    \n",
        "    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n",
        "    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n",
        "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
        "    #catcols = ['id', 'item_id', 'dept_id', 'cat_id', 'state_id']\n",
        "    dtype = {numcol:\"float32\" for numcol in numcols} \n",
        "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
        "    dt = pd.read_csv(dataset_path + \"sales_train_evaluation.csv\", \n",
        "                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n",
        "    print(dt.shape)\n",
        "    for col in catcols:\n",
        "        if col != \"id\":\n",
        "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
        "            dt[col] -= dt[col].min()\n",
        "    \n",
        "    if not is_train:\n",
        "        for day in range(tr_last+1, tr_last+ 28 +1):\n",
        "            dt[f\"d_{day}\"] = np.nan\n",
        "    \n",
        "    dt = pd.melt(dt,\n",
        "                  id_vars = catcols,\n",
        "                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n",
        "                  var_name = \"d\",\n",
        "                  value_name = \"sales\")\n",
        "    dt = dt.merge(cal, on= \"d\", copy = False)\n",
        "    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
        "    print(dt.shape)\n",
        "    return dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtmFZ8VXCC2P",
        "colab_type": "text"
      },
      "source": [
        "Lagging and Rolling mean of the lags are computed. Some date features have also been added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOUiG7Rlu2jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_fea(dt):\n",
        "    lags = [1, 7, 14, 28]\n",
        "    lag_cols = [f\"lag_{lag}\" for lag in lags]\n",
        "    lag_price = [f\"price_lag_{lag}\" for lag in lags]\n",
        "    for lag, lag_col, price_lag in zip(lags, lag_cols, lag_price):\n",
        "        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
        "        dt[price_lag] = dt[[\"id\",\"sell_price\"]].groupby(\"id\")[\"sell_price\"].shift(lag)\n",
        "    print(\"LAGS DONE\")\n",
        "    wins = [1, 7, 14]\n",
        "    for win in wins :\n",
        "        for lag,lag_col, price_lag in zip(lags, lag_cols, lag_price):\n",
        "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
        "            dt[f\"price_rmean_{lag}_{win}\"] = dt[[\"id\", price_lag]].groupby(\"id\")[price_lag].transform(lambda x : x.rolling(win).mean())\n",
        "    \n",
        "    print(\"ROLLINGS DONE\")\n",
        "    date_features = {\n",
        "        \n",
        "        \"wday\": \"weekday\",\n",
        "        \"week\": \"weekofyear\",\n",
        "        \"month\": \"month\",\n",
        "        \"year\": \"year\",\n",
        "        \"mday\": \"day\",\n",
        "        #\"quarter\" : \"quarter\",\n",
        "#         \"ime\": \"is_month_end\",\n",
        "#         \"ims\": \"is_month_start\",\n",
        "    }\n",
        "    \n",
        "#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n",
        "    \n",
        "    for date_feat_name, date_feat_func in date_features.items():\n",
        "        if date_feat_name in dt.columns:\n",
        "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
        "        else:\n",
        "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJG2A-33u42B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIRST_DAY = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld8JeANDu7Od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5261809e-3041-4957-bdc6-a99ccc20c973"
      },
      "source": [
        "df = create_dt(is_train=True, first_day= FIRST_DAY)\n",
        "df.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30490, 1947)\n",
            "(46881677, 22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(30490, 1570)\\n(40718219, 22)\\n(40718219, 22)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q9_1Ghag_iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(['wm_yr_wk', 'weekday', 'd'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl1xHdkaCWOF",
        "colab_type": "text"
      },
      "source": [
        "CSV file for each store is created and stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvoVsVm_QtXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for 10 models\n",
        "store_ids = [x for x in range(10)]\n",
        "for store_id in store_ids:\n",
        "    df_part = df[df['store_id']==store_id]\n",
        "    print(store_id)\n",
        "    state = list(set(list(df_part['id'])))[0].split('_')[3]\n",
        "    snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "    snap_cols.remove('snap_'+state)\n",
        "    snap_cols.append('state_id')\n",
        "    print(snap_cols)\n",
        "    use_cols = df_part.columns[~df_part.columns.isin(snap_cols)]\n",
        "    df_part = df_part[use_cols]\n",
        "    create_fea(df_part)\n",
        "    snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "    out_name = \"input_\"+str(store_id)+ \".csv\"\n",
        "    print(df_part.shape)\n",
        "    df_part.dropna(inplace=True)\n",
        "    print(df_part.shape)\n",
        "    df_part.to_csv(dataset_path + out_name, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjvbB6_kCcGE",
        "colab_type": "text"
      },
      "source": [
        "CSV file for each store and category is created and stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh0UHWQZjIaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for 30 models\n",
        "store_ids = [x for x in range(10)]\n",
        "cat_ids = [x for x in range(3)]\n",
        "for store_id in store_ids:\n",
        "  for cat_id in cat_ids:\n",
        "    #if store_id == 8 and cat_id == 2:\n",
        "      df_part = df[(df['store_id']==store_id) & (df['cat_id'] == cat_id)]\n",
        "      print(store_id, cat_id)\n",
        "      state = list(set(list(df_part['id'])))[0].split('_')[3]\n",
        "      snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "      if cat_id == 2:\n",
        "        snap_cols.remove('snap_'+state)\n",
        "      snap_cols.append('state_id')\n",
        "      print(snap_cols)\n",
        "      use_cols = df_part.columns[~df_part.columns.isin(snap_cols)]\n",
        "      df_part = df_part[use_cols]\n",
        "      create_fea(df_part)\n",
        "      snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "      out_name = \"input_\"+str(store_id) +\"_\"+str(cat_id)+ \".csv\"\n",
        "      print(df_part.shape)\n",
        "      df_part.dropna(inplace=True)\n",
        "      print(df_part.shape)\n",
        "      \n",
        "      df_part.to_csv(dataset_path +\"input/\" + out_name, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9oK3wi0Chv8",
        "colab_type": "text"
      },
      "source": [
        "CSV file for each state and department is created and stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClN3g2nPCQtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for 21 models\n",
        "state_ids = [x for x in range(3)]\n",
        "dept_ids = [x for x in range(7)]\n",
        "for state_id in state_ids:\n",
        "  for dept_id in dept_ids:\n",
        "    #if state_id == 1 and dept_id ==2:\n",
        "      df_part = df[(df['state_id']==state_id) & (df['dept_id'] == dept_id)]\n",
        "      print(state_id, dept_id)\n",
        "      snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "      state = list(set(list(df_part['id'])))[0].split('_')[3]\n",
        "      if set(df_part['cat_id']) == {2}:\n",
        "        snap_cols.remove('snap_'+state)\n",
        "      print(snap_cols)\n",
        "      use_cols = df_part.columns[~df_part.columns.isin(snap_cols)]\n",
        "      df_part = df_part[use_cols]\n",
        "      create_fea(df_part)\n",
        "      snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "      out_name = \"input_\"+str(state_id) +\"_\"+str(dept_id)+ \".csv\"\n",
        "      print(df_part.shape)\n",
        "      df_part.dropna(inplace=True)\n",
        "      print(df_part.shape)\n",
        "      df_part.to_csv(dataset_path + \"input/\" + out_name, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtOY_isyCnV7",
        "colab_type": "text"
      },
      "source": [
        "Params for LGB are created. Training for different CSVs were done with different set of parameters, according to the validation score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsUp29au0IlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params1 = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.5,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'lambda_l2' : 0.1,\n",
        "                    #'lambda_l1' : 0.1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 2500,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': 1,\n",
        "\n",
        "                   #'early_stopping_round' : 500\n",
        "                } \n",
        "params2 = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.5,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'lambda_l2' : 0.1,\n",
        "                    #'lambda_l1' : 0.1,\n",
        "                    'learning_rate': 0.075,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 5000,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': 1,\n",
        "\n",
        "                   'early_stopping_round' : 500\n",
        "                }  \n",
        "params3 = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.5,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'lambda_l2' : 0.1,\n",
        "                    #'lambda_l1' : 0.1,\n",
        "                    'learning_rate': 0.07,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 4800,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': 1,\n",
        "\n",
        "                   #'early_stopping_round' : 500\n",
        "                } \n",
        "params4 = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.5,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'lambda_l2' : 0.1,\n",
        "                    #'lambda_l1' : 0.1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 5000,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': 1,\n",
        "\n",
        "                   'early_stopping_round' : 500\n",
        "                }                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AxqQKzz2FaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaZuMuFkC4v9",
        "colab_type": "text"
      },
      "source": [
        "Training is done. While predicting the entire data is used for prediction as it contains more information. Validation is used only for setting the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbL7R-UqRaVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for 21 models\n",
        "cat_feats = ['item_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
        "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"Discount\", \"quarter\", \"state_id\", \"cat_id\", \"dept_id\"]\n",
        "np.random.seed(0)\n",
        "for i in range(3):\n",
        "  for j in range(7):\n",
        "      print(i, j)\n",
        "      df_inp = pd.read_csv(dataset_path+\"input/\"+ \"input_\"+str(i) +\"_\"+str(j)+\".csv\")\n",
        "      y_train = df_inp[\"sales\"]\n",
        "      #y_val = X_val_orig[\"sales\"]\n",
        "      train_cols = df_inp.columns[~df_inp.columns.isin(useless_cols)]\n",
        "      X_train = df_inp[train_cols]\n",
        "      #X_val = X_val_orig[train_cols]\n",
        "      #train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_feats, free_raw_data=False)\n",
        "      #fake_valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_feats, free_raw_data=False)\n",
        "      '''num_rows = X_train.shape[0]\n",
        "      val_size = int(0.2*num_rows)\n",
        "      fake_valid_inds = np.random.choice(X_train.index.values, val_size, replace = False)\n",
        "      train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
        "      train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
        "                              categorical_feature=cat_feats, free_raw_data=False)\n",
        "      fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n",
        "                                    categorical_feature=cat_feats,free_raw_data=False)\n",
        "      del df_inp, X_train, y_train, fake_valid_inds, train_inds'''\n",
        "      \n",
        "      train_data = lgb.Dataset(X_train , label = y_train, \n",
        "                              categorical_feature=cat_feats, free_raw_data=False)\n",
        "                              \n",
        "      del df_inp, X_train, y_train\n",
        "      gc.collect()\n",
        "\n",
        "      #m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100) \n",
        "\n",
        "      m_lgb = lgb.train(params, train_data,valid_sets = [train_data], verbose_eval=100) \n",
        "      model_name = dataset_path+\"models/\" + str(i)+str(j)+\".bin\"\n",
        "      pickle.dump(m_lgb, open(model_name, 'wb'))\n",
        "      del m_lgb\n",
        "      gc.collect()\n",
        "      #m_lgb.save_model(dataset_path+str(i)+\".lgb\")'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxw1QE1sebST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for 10 models\n",
        "cat_feats = ['item_id', 'dept_id', 'cat_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
        "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"store_id\", \"Discount\", \"quarter\"]\n",
        "np.random.seed(10)\n",
        "for i in range(10):\n",
        "    print(\"store-\" + str(i))\n",
        "    df_inp = pd.read_csv(dataset_path+\"input_\"+str(i)+\".csv\")\n",
        "    y_train = df_inp[\"sales\"]\n",
        "    #y_val = X_val_orig[\"sales\"]\n",
        "    train_cols = df_inp.columns[~df_inp.columns.isin(useless_cols)]\n",
        "    X_train = df_inp[train_cols]\n",
        "    #X_val = X_val_orig[train_cols]\n",
        "    print(X_train.shape)\n",
        "    #train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_feats, free_raw_data=False)\n",
        "    #fake_valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_feats, free_raw_data=False)\n",
        "    '''\n",
        "    num_rows = X_train.shape[0]\n",
        "    val_size = int(0.2*num_rows)\n",
        "    fake_valid_inds = np.random.choice(X_train.index.values, val_size, replace = False)\n",
        "    train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
        "    train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
        "                            categorical_feature=cat_feats, free_raw_data=False)\n",
        "    fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n",
        "                                  categorical_feature=cat_feats,free_raw_data=False)\n",
        "    del df_inp, X_train, y_train, fake_valid_inds, train_inds'''\n",
        "\n",
        "    train_data = lgb.Dataset(X_train , label = y_train, \n",
        "                            categorical_feature=cat_feats, free_raw_data=False)\n",
        "    del df_inp, X_train, y_train\n",
        "    gc.collect()\n",
        "\n",
        "    #m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100) \n",
        "\n",
        "    m_lgb = lgb.train(params, train_data,valid_sets = [train_data], verbose_eval=100) \n",
        "    model_name = dataset_path+str(i)+\".bin\"\n",
        "    pickle.dump(m_lgb, open(model_name, 'wb'))\n",
        "    del m_lgb\n",
        "    gc.collect()\n",
        "    #m_lgb.save_model(dataset_path+str(i)+\".lgb\")'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMsHynnOwpaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#foe 30 models\n",
        "import pickle\n",
        "cat_feats = ['item_id', 'dept_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
        "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"store_id\", \"Discount\", \"quarter\", \"cat_id\"]\n",
        "np.random.seed(10)\n",
        "for i in range(10):\n",
        "  for j in range(3):\n",
        "    if i == 2 and j == 2:\n",
        "      params = params3\n",
        "      \n",
        "    elif (i == 7 or i == 9 or i == 0) and j == 2:\n",
        "      params = params2\n",
        "    elif (i == 1) and j == 2:\n",
        "      params = params4\n",
        "    else:\n",
        "      params = params1\n",
        "      \n",
        "\n",
        "    print(\"store-\" + str(i) + \"cat-\" + str(j) + str(params['n_estimators']))\n",
        "    df_inp = pd.read_csv(dataset_path+\"input/\"+\"input_\"+str(i) +\"_\"+str(j)+\".csv\")\n",
        "    y_train = df_inp[\"sales\"]\n",
        "    #y_val = X_val_orig[\"sales\"]\n",
        "    train_cols = df_inp.columns[~df_inp.columns.isin(useless_cols)]\n",
        "    X_train = df_inp[train_cols]\n",
        "    #X_val = X_val_orig[train_cols]\n",
        "    print(X_train.shape)\n",
        "    #train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_feats, free_raw_data=False)\n",
        "    #fake_valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_feats, free_raw_data=False)\n",
        "    '''\n",
        "    num_rows = X_train.shape[0]\n",
        "    val_size = int(0.2*num_rows)\n",
        "    fake_valid_inds = np.random.choice(X_train.index.values, val_size, replace = False)\n",
        "    train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
        "    train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
        "                            categorical_feature=cat_feats, free_raw_data=False)\n",
        "    fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n",
        "                                  categorical_feature=cat_feats,free_raw_data=False)\n",
        "    del df_inp, X_train, y_train, fake_valid_inds, train_inds'''\n",
        "\n",
        "    train_data = lgb.Dataset(X_train , label = y_train, \n",
        "                            categorical_feature=cat_feats, free_raw_data=False)\n",
        "    del df_inp, X_train, y_train\n",
        "    gc.collect()\n",
        "\n",
        "    #m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100) \n",
        "\n",
        "    m_lgb = lgb.train(params, train_data,valid_sets = [train_data], verbose_eval=100) \n",
        "    model_name = dataset_path+\"models/\" + str(i)+str(j)+\".bin\"\n",
        "    pickle.dump(m_lgb, open(model_name, 'wb'))\n",
        "    del m_lgb\n",
        "    gc.collect()\n",
        "    #m_lgb.save_model(dataset_path+str(i)+\".lgb\")'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXg0G8RO8xiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for 10 models\n",
        "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"store_id\", \"Discount\", \"quarter\"]\n",
        "def get_test_data(df, ind):\n",
        "  test_df = df.loc[df.store_id == ind]\n",
        "  #test_df = test_df.drop(test_df.columns[[0]], axis=1)\n",
        "  state = list(set(list(test_df['id'])))[0].split('_')[3]\n",
        "  snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "  snap_cols.remove('snap_'+state)\n",
        "  #snap_cols.remove('snap_'+state)\n",
        "  snap_cols.append('state_id')\n",
        "  snap_cols.extend(useless_cols)\n",
        "  #print(snap_cols)\n",
        "  use_cols = test_df.columns[~test_df.columns.isin(snap_cols)]\n",
        "  test_df = test_df[use_cols]\n",
        "  return test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GETj8xYLtp4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for 30 models\n",
        "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"store_id\", \"Discount\", \"quarter\", \"cat_id\"]\n",
        "def get_test_data(df, ind, cat_ind):\n",
        "  test_df = df.loc[(df.store_id == ind) & (df.cat_id == cat_ind)]\n",
        "  #test_df = test_df.drop(test_df.columns[[0]], axis=1)\n",
        "  state = list(set(list(test_df['id'])))[0].split('_')[3]\n",
        "  snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "  if cat_ind == 2:\n",
        "    snap_cols.remove('snap_'+state)\n",
        "  #snap_cols.remove('snap_'+state)\n",
        "  snap_cols.append('state_id')\n",
        "  snap_cols.extend(useless_cols)\n",
        "  #print(snap_cols)\n",
        "  use_cols = test_df.columns[~test_df.columns.isin(snap_cols)]\n",
        "  test_df = test_df[use_cols]\n",
        "  return test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg80AGG-XbT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for 21 models\n",
        "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"Discount\", \"quarter\", \"state_id\", \"cat_id\", \"dept_id\"]\n",
        "def get_test_data(df, ind, dept_ind):\n",
        "  test_df = df.loc[(df.state_id == ind) & (df.dept_id == dept_ind)]\n",
        "  #test_df = test_df.drop(test_df.columns[[0]], axis=1)\n",
        "  state = list(set(list(test_df['id'])))[0].split('_')[3]\n",
        "  snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
        "  if set(test_df['cat_id']) == {2}:\n",
        "    snap_cols.remove('snap_'+state)\n",
        "  #snap_cols.remove('snap_'+state)\n",
        "  snap_cols.extend(useless_cols)\n",
        "  #print(snap_cols)\n",
        "  use_cols = test_df.columns[~test_df.columns.isin(snap_cols)]\n",
        "  test_df = test_df[use_cols]\n",
        "  return test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USFTLKgpDEjZ",
        "colab_type": "text"
      },
      "source": [
        "Prediction is done. Recursive feature is used in the prediction task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9LEnqQ2YWGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#for 21 models\n",
        "import pickle\n",
        "#alphas = [1.028, 1.023, 1.018]\n",
        "alphas = [1]\n",
        "weights = [1/len(alphas)]*len(alphas)\n",
        "sub = 0.\n",
        "\n",
        "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
        "\n",
        "    te = create_dt(False)\n",
        "    te = te.drop(['wm_yr_wk', 'weekday', 'd'], axis=1)\n",
        "    #te = te[(te.state_id == 0) & ((te.dept_id == 6) | (te.dept_id == 2))]\n",
        "    #te = te[(te.state_id == 0) & (te.dept_id == 2)]\n",
        "    #te['sales_max'] = te[['id', 'sales']].groupby('id')['sales'].transform('max')\n",
        "    #te['sales_min'] = te[['id', 'sales']].groupby('id')['sales'].transform('min')\n",
        "    #te['sales'] = (te['sales']-te['sales_min'])/te['sales_max']\n",
        "    #te = te.drop(['sell_price', 'price_max'], axis=1)\n",
        "    #te.rename(columns={'price_norm':'sell_price'}, inplace=True)\n",
        "    cols = [f\"F{i}\" for i in range(1,29)]\n",
        "    for tdelta in range(28):\n",
        "        day = fday + timedelta(days=tdelta)\n",
        "        print(tdelta, day)\n",
        "        #tst = te[((te.date >= fday - timedelta(days=max_lags)) & (te.date < fday)) | (te.date==day) ].copy()\n",
        "        tst = te[((te.date >= fday - timedelta(days=max_lags)) & (te.date <= day))].copy()\n",
        "        create_fea(tst)\n",
        "        tst = tst.loc[tst.date == day]\n",
        "\n",
        "        \n",
        "        for i in range(3):\n",
        "          for j in range(7):\n",
        "            #if (i == 0 and j == 2): #or (i == 0 and j == 2):\n",
        "              tst_df = get_test_data(tst, i, j)\n",
        "              print(i, j, tst_df.shape)\n",
        "              model_path = dataset_path + \"models/\"+str(i) +str(j)+ \".bin\"\n",
        "              estimator = pickle.load(open(model_path, 'rb'))\n",
        "              res = alpha*estimator.predict(tst_df)\n",
        "              te.loc[(te.date == day) & (te.state_id == i) & (te.dept_id==j), \"sales\"] = res\n",
        "        #te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
        "\n",
        "\n",
        "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
        "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
        "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
        "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
        "    te_sub.fillna(0., inplace = True)\n",
        "    te_sub.sort_values(\"id\", inplace = True)\n",
        "    te_sub.reset_index(drop=True, inplace = True)\n",
        "    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
        "    if icount == 0 :\n",
        "        sub = te_sub\n",
        "        sub[cols] *= weight\n",
        "    else:\n",
        "        sub[cols] += te_sub[cols]*weight\n",
        "    print(icount, alpha, weight)\n",
        "\n",
        "\n",
        "sub2 = sub.copy()\n",
        "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
        "sub.to_csv(dataset_path+\"submission.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lKY_hdQpbIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#for 10 models\n",
        "import pickle\n",
        "#alphas = [1.028, 1.023, 1.018]\n",
        "alphas = [1]\n",
        "weights = [1/len(alphas)]*len(alphas)\n",
        "sub = 0.\n",
        "\n",
        "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
        "\n",
        "    te = create_dt(False)\n",
        "    te = te.drop(['wm_yr_wk', 'weekday', 'd'], axis=1)\n",
        "    cols = [f\"F{i}\" for i in range(1,29)]\n",
        "\n",
        "    for tdelta in range(28):\n",
        "        day = fday + timedelta(days=tdelta)\n",
        "        print(tdelta, day)\n",
        "        #tst = te[((te.date >= fday - timedelta(days=max_lags)) & (te.date < fday)) | (te.date==day) ].copy()\n",
        "        tst = te[((te.date >= fday - timedelta(days=max_lags)) & (te.date <= day))].copy()\n",
        "        create_fea(tst)\n",
        "        tst = tst.loc[tst.date == day]\n",
        "\n",
        "        \n",
        "        for i in range(10):\n",
        "            tst_df = get_test_data(tst, i)\n",
        "            print(i, tst_df.shape)\n",
        "            model_path = dataset_path + str(i) + \".bin\"\n",
        "            estimator = pickle.load(open(model_path, 'rb'))\n",
        "            res = alpha*estimator.predict(tst_df)\n",
        "            te.loc[(te.date == day) & (te.store_id == i) , \"sales\"] = res\n",
        "        #te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
        "\n",
        "\n",
        "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
        "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
        "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
        "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
        "    te_sub.fillna(0., inplace = True)\n",
        "    te_sub.sort_values(\"id\", inplace = True)\n",
        "    te_sub.reset_index(drop=True, inplace = True)\n",
        "    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
        "    if icount == 0 :\n",
        "        sub = te_sub\n",
        "        sub[cols] *= weight\n",
        "    else:\n",
        "        sub[cols] += te_sub[cols]*weight\n",
        "    print(icount, alpha, weight)\n",
        "\n",
        "\n",
        "sub2 = sub.copy()\n",
        "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
        "sub.to_csv(dataset_path+\"submission.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2wpHhTlJ2wN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#for 30 models\n",
        "import pickle\n",
        "#alphas = [1.028, 1.023, 1.018]\n",
        "alphas = [1]\n",
        "weights = [1/len(alphas)]*len(alphas)\n",
        "sub = 0.\n",
        "\n",
        "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
        "\n",
        "    te = create_dt(False)\n",
        "    te = te.drop(['wm_yr_wk', 'weekday', 'd'], axis=1)\n",
        "    #te = te[(te.store_id == 0) & ((te.store_id == 6) | (te.dept_id == 2))]\n",
        "    #te = te[(te.store_id == 7) & (te.cat_id == 2)]\n",
        "    #te['sales_max'] = te[['id', 'sales']].groupby('id')['sales'].transform('max')\n",
        "    #te['sales_min'] = te[['id', 'sales']].groupby('id')['sales'].transform('min')\n",
        "    #te['sales'] = (te['sales']-te['sales_min'])/te['sales_max']\n",
        "    #te = te.drop(['sell_price', 'price_max'], axis=1)\n",
        "    #te.rename(columns={'price_norm':'sell_price'}, inplace=True)\n",
        "    cols = [f\"F{i}\" for i in range(1,29)]\n",
        "    for tdelta in range(28):\n",
        "        day = fday + timedelta(days=tdelta)\n",
        "        print(tdelta, day)\n",
        "        #tst = te[((te.date >= fday - timedelta(days=max_lags)) & (te.date < fday)) | (te.date==day) ].copy()\n",
        "        tst = te[((te.date >= fday - timedelta(days=max_lags)) & (te.date <= day))].copy()\n",
        "        create_fea(tst)\n",
        "        tst = tst.loc[tst.date == day]\n",
        "\n",
        "        \n",
        "        for i in range(10):\n",
        "          for j in range(3):\n",
        "            #if (i == 7 and j == 2): #or (i == 0 and j == 2):\n",
        "              tst_df = get_test_data(tst, i, j)\n",
        "              print(i, j, tst_df.shape)\n",
        "              model_path = dataset_path + \"models/\" + str(i) +str(j)+ \".bin\"\n",
        "              estimator = pickle.load(open(model_path, 'rb'))\n",
        "              res = alpha*estimator.predict(tst_df)\n",
        "              te.loc[(te.date == day) & (te.store_id == i) & (te.cat_id==j), \"sales\"] = res\n",
        "        #te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
        "\n",
        "\n",
        "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
        "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
        "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
        "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
        "    te_sub.fillna(0., inplace = True)\n",
        "    te_sub.sort_values(\"id\", inplace = True)\n",
        "    te_sub.reset_index(drop=True, inplace = True)\n",
        "    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
        "    if icount == 0 :\n",
        "        sub = te_sub\n",
        "        sub[cols] *= weight\n",
        "    else:\n",
        "        sub[cols] += te_sub[cols]*weight\n",
        "    print(icount, alpha, weight)\n",
        "\n",
        "\n",
        "sub2 = sub.copy()\n",
        "#sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "sub2[\"id\"] = sub2[\"id\"].str.replace(\"evaluation$\", \"validation\")\n",
        "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
        "sub.to_csv(dataset_path+\"sub.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLBKVXXPYTSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}